{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationConfig:\n",
    "    accepted_data_dir: Path\n",
    "    rejected_data_dir: Path\n",
    "    file_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationArtifact:\n",
    "    accepted_file_path: Path\n",
    "    rejected_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consumerComplaint.constants.training_pipeline_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from consumerComplaint.constants.training_pipeline_config.data_validation import *\n",
    "from consumerComplaint.constants.training_pipeline_config import *\n",
    "from consumerComplaint.constants import TIMESTAMP\n",
    "from consumerComplaint.constants import *\n",
    "from consumerComplaint.logger import logger\n",
    "from consumerComplaint.exception import ConsumerComplaintException\n",
    "from consumerComplaint.entity.metadata_entity import DataIngestionMetadata\n",
    "from consumerComplaint.entity.config_entity import DataIngestionConfig, TrainingPipelineConfig, DataValidationConfig\n",
    "\n",
    "\n",
    "\n",
    "class FinanceConfig:\n",
    "    def __init__(self, pipeline_name=PIPELINE_NAME, timestamp=TIMESTAMP):\n",
    "        \"\"\"\n",
    "        Organization: iNeuron Intelligence Private Limited\n",
    "\n",
    "        \"\"\"\n",
    "        self.timestamp = timestamp\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.pipeline_config = self.get_pipeline_config()\n",
    "\n",
    "    def get_pipeline_config(self) -> TrainingPipelineConfig:\n",
    "        \"\"\"\n",
    "        This function will provide pipeline config information\n",
    "\n",
    "\n",
    "        returns > PipelineConfig = namedtuple(\"PipelineConfig\", [\"pipeline_name\", \"artifact_dir\"])\n",
    "        \"\"\"\n",
    "        try:\n",
    "            artifact_dir = PIPELINE_ARTIFACT_DIR\n",
    "            pipeline_config = TrainingPipelineConfig(pipeline_name=self.pipeline_name,\n",
    "                                                     artifact_dir=artifact_dir)\n",
    "\n",
    "            logger.info(f\"Pipeline configuration: {pipeline_config}\")\n",
    "\n",
    "            return pipeline_config\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "\n",
    "    def get_data_ingestion_config(self, \n",
    "                                  from_date=DATA_INGESTION_MIN_START_DATE, \n",
    "                                  to_date=None)-> DataIngestionConfig:\n",
    "        try:\n",
    "            min_start_date = datetime.strptime(DATA_INGESTION_MIN_START_DATE, \"%Y-%m-%d\")\n",
    "            from_date_obj = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "            if from_date_obj < min_start_date:\n",
    "                from_date = DATA_INGESTION_MIN_START_DATE\n",
    "            if to_date is None:\n",
    "                to_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            \"\"\"\n",
    "            master directory for data ingestion\n",
    "            we will store metadata information and ingested file to avoid redundant download\n",
    "            \"\"\"\n",
    "            data_ingestion_master_dir = Path(self.pipeline_config.artifact_dir) / DATA_INGESTION_DIR\n",
    "\n",
    "            # Create a time-based directory for each run\n",
    "            data_ingestion_dir = data_ingestion_master_dir / self.timestamp\n",
    "\n",
    "            metadata_file_path = data_ingestion_master_dir / DATA_INGESTION_METADATA_FILE_NAME\n",
    "\n",
    "            data_ingestion_metadata = DataIngestionMetadata(metadata_file_path=metadata_file_path)\n",
    "\n",
    "            if data_ingestion_metadata.is_metadata_file_present:\n",
    "                metadata_info = data_ingestion_metadata.get_metadata_info()\n",
    "                from_date = metadata_info.to_date\n",
    "\n",
    "            data_ingestion_config = DataIngestionConfig(\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                data_ingestion_dir=data_ingestion_dir,\n",
    "                download_dir=os.path.join(data_ingestion_dir, DATA_INGESTION_DOWNLOADED_DATA_DIR),\n",
    "                file_name=DATA_INGESTION_FILE_NAME,\n",
    "                feature_store_dir=os.path.join(data_ingestion_master_dir, DATA_INGESTION_FEATURE_STORE_DIR),\n",
    "                failed_dir=os.path.join(data_ingestion_dir, DATA_INGESTION_FAILED_DIR),\n",
    "                metadata_file_path=metadata_file_path,\n",
    "                datasource_url=DATA_INGESTION_DATA_SOURCE_URL\n",
    "\n",
    "            )\n",
    "            logger.info(f\"Data ingestion config: {data_ingestion_config}\")\n",
    "            return data_ingestion_config\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "        \n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            data_validation_master_dir = Path(self.pipeline_config.artifact_dir) / DATA_VALIDATION_DIR\n",
    "            data_validation_dir = data_validation_master_dir / self.timestamp\n",
    "\n",
    "            accepted_data_dir = Path(data_validation_dir) / DATA_VALIDATION_ACCEPTED_DATA_DIR\n",
    "            \n",
    "            rejected_data_dir = Path(data_validation_dir) /DATA_VALIDATION_REJECTED_DATA_DIR\n",
    "\n",
    "            data_preprocessing_config = DataValidationConfig(\n",
    "                accepted_data_dir=accepted_data_dir,\n",
    "                rejected_data_dir=rejected_data_dir,\n",
    "                file_name=DATA_VALIDATION_FILE_NAME\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Data preprocessing config: {data_preprocessing_config}\")\n",
    "\n",
    "            return data_preprocessing_config\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FinanceConfig()\n",
    "data_validation_config = config.get_data_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataValidationConfig(accepted_data_dir=PosixPath('/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/research/consumer_artifact/data_validation/20230922_023149/accepted_data'), rejected_data_dir=PosixPath('/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/research/consumer_artifact/data_validation/20230922_023149/rejected_data'), file_name='consumer_complaint')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKIA6EOXBVIUU4TOWIFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 01:32:39 WARN Utils: Your hostname, Dell resolves to a loopback address: 127.0.1.1; using 192.168.29.77 instead (on interface wlp0s20f3)\n",
      "23/09/23 01:32:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/suyodhan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/suyodhan/.ivy2/jars\n",
      "com.amazonaws#aws-java-sdk added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-81972d2b-3503-4806-bdea-5b755d181b15;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2 in central\n",
      "\tfound commons-codec#commons-codec;1.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.1.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.1.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.1.1 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\t[2.12.5] joda-time#joda-time;[2.2,)\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.3 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.3 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      ":: resolution report :: resolve 4738ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.3 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.1.1 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2 by [org.apache.httpcomponents#httpclient;4.2.5] in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2 by [org.apache.httpcomponents#httpcore;4.2.5] in [default]\n",
      "\tcommons-codec#commons-codec;1.6 by [commons-codec#commons-codec;1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.3 by [commons-codec#commons-codec;1.4] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.1.1 by [com.fasterxml.jackson.core#jackson-core;2.2.3] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.1.1 by [com.fasterxml.jackson.core#jackson-databind;2.2.3] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.1.1 by [com.fasterxml.jackson.core#jackson-annotations;2.2.3] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   76  |   1   |   0   |   8   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-81972d2b-3503-4806-bdea-5b755d181b15\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/14ms)\n",
      "23/09/23 01:32:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from typing import List, Dict\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "from consumerComplaint.config.spark_manager import spark_session\n",
    "from consumerComplaint.entity.artifact_entity import DataIngestionArtifact\n",
    "from consumerComplaint.entity.config_entity import DataValidationConfig\n",
    "from consumerComplaint.entity.schema import FinanceDataSchema\n",
    "from consumerComplaint.exception import ConsumerComplaintException\n",
    "from consumerComplaint.logger import logger\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "from consumerComplaint.entity.artifact_entity import DataValidationArtifact\n",
    "\n",
    "COMPLAINT_TABLE = \"complaint\"\n",
    "ERROR_MESSAGE = \"error_msg\"\n",
    "MissingReport = namedtuple(\"MissingReport\", [\"total_row\", \"missing_row\", \"missing_percentage\"])\n",
    "\n",
    "\n",
    "class DataValidation(FinanceDataSchema):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_validation_config: DataValidationConfig,\n",
    "                 data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 table_name: str = COMPLAINT_TABLE,\n",
    "                 schema=FinanceDataSchema()\n",
    "                 ):\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.data_ingestion_artifact: DataIngestionArtifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self.table_name = table_name\n",
    "            self.schema = schema\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys) from e\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        try:\n",
    "            dataframe: DataFrame = spark_session.read.parquet(\n",
    "                self.data_ingestion_artifact.feature_store_file_path\n",
    "            ).limit(10000)\n",
    "            logger.info(f\"Data frame is created using file: {self.data_ingestion_artifact.feature_store_file_path}\")\n",
    "            logger.info(f\"Number of row: {dataframe.count()} and column: {len(dataframe.columns)}\")\n",
    "            #dataframe, _ = dataframe.randomSplit([0.001, 0.999])\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_missing_report(dataframe: DataFrame, ) -> Dict[str, MissingReport]:\n",
    "        try:\n",
    "            missing_report: Dict[str:MissingReport] = dict()\n",
    "            logger.info(f\"Preparing missing reports for each column\")\n",
    "            number_of_row = dataframe.count()\n",
    "\n",
    "            for column in dataframe.columns:\n",
    "                missing_row = dataframe.filter(f\"{column} is null\").count()\n",
    "                missing_percentage = (missing_row * 100) / number_of_row\n",
    "                missing_report[column] = MissingReport(total_row=number_of_row,\n",
    "                                                       missing_row=missing_row,\n",
    "                                                       missing_percentage=missing_percentage\n",
    "                                                       )\n",
    "            logger.info(f\"Missing report prepared: {missing_report}\")\n",
    "            return missing_report\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def get_unwanted_and_high_missing_value_columns(self, dataframe: DataFrame, threshold: float = 0.2) -> List[str]:\n",
    "        try:\n",
    "            missing_report: Dict[str, MissingReport] = self.get_missing_report(dataframe=dataframe)\n",
    "\n",
    "            unwanted_column: List[str] = self.schema.unwanted_columns\n",
    "            for column in missing_report:\n",
    "                if missing_report[column].missing_percentage > (threshold * 100):\n",
    "                    unwanted_column.append(column)\n",
    "                    logger.info(f\"Missing report {column}: [{missing_report[column]}]\")\n",
    "            unwanted_column = list(set(unwanted_column))\n",
    "            return unwanted_column\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def drop_unwanted_columns(self, dataframe: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            unwanted_columns: List = self.get_unwanted_and_high_missing_value_columns(dataframe=dataframe, )\n",
    "            logger.info(f\"Dropping feature: {','.join(unwanted_columns)}\")\n",
    "            unwanted_dataframe: DataFrame = dataframe.select(unwanted_columns)\n",
    "\n",
    "            unwanted_dataframe = unwanted_dataframe.withColumn(ERROR_MESSAGE, lit(\"Contains many missing values\"))\n",
    "\n",
    "            rejected_dir = Path(self.data_validation_config.rejected_data_dir) / \"missing_data\"\n",
    "            rejected_dir.mkdir(exist_ok=True)\n",
    "            file_path =Path(rejected_dir) / self.data_validation_config.file_name\n",
    "\n",
    "            logger.info(f\"Writing dropped column into file: [{file_path}]\")\n",
    "            unwanted_dataframe.write.mode(\"append\").parquet(file_path)\n",
    "            dataframe: DataFrame = dataframe.drop(*unwanted_columns)\n",
    "            logger.info(f\"Remaining number of columns: [{dataframe.columns}]\")\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unique_values_of_each_column(dataframe: DataFrame) -> None:\n",
    "        try:\n",
    "            for column in dataframe.columns:\n",
    "                n_unique: int = dataframe.select(col(column)).distinct().count()\n",
    "                n_missing: int = dataframe.filter(col(column).isNull()).count()\n",
    "                missing_percentage: float = (n_missing * 100) / dataframe.count()\n",
    "                logger.info(f\"Column: {column} contains {n_unique} value and missing perc: {missing_percentage} %.\")\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def is_required_columns_exist(self, dataframe: DataFrame):\n",
    "        try:\n",
    "            columns = list(filter(lambda x: x in self.schema.required_columns,\n",
    "                                  dataframe.columns))\n",
    "\n",
    "            if len(columns) != len(self.schema.required_columns):\n",
    "                raise Exception(f\"Required column missing\\n\\\n",
    "                 Expected columns: {self.schema.required_columns}\\n\\\n",
    "                 Found columns: {columns}\\\n",
    "                 \")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    # def drop_row_without_target_label(self, dataframe: DataFrame) -> DataFrame:\n",
    "    #     try:\n",
    "    #         dropped_rows = \"dropped_row\"\n",
    "    #         total_rows: int = dataframe.count()\n",
    "    #         logger.info(f\"Number of row: {total_rows} \")\n",
    "    #\n",
    "    #         # Drop row if target value is unknown\n",
    "    #         logger.info(f\"Dropping rows without target value.\")\n",
    "    #         unlabelled_dataframe: DataFrame = dataframe.filter(f\"{self.target_column}== 'N/A'\")\n",
    "    #\n",
    "    #         rejected_dir = os.path.join(self.data_validation_config.rejected_data_dir, dropped_rows)\n",
    "    #         os.makedirs(rejected_dir, exist_ok=True)\n",
    "    #         file_path = os.path.join(rejected_dir, self.data_validation_config.file_name)\n",
    "    #\n",
    "    #         unlabelled_dataframe = unlabelled_dataframe.withColumn(ERROR_MESSAGE, lit(\"Dropped row as target label is \"\n",
    "    #                                                                                   \"unknown\"))\n",
    "    #\n",
    "    #         logger.info(f\"Unlabelled data has row: [{unlabelled_dataframe.count()}] and columns:\"\n",
    "    #                     f\" [{len(unlabelled_dataframe.columns)}]\")\n",
    "    #\n",
    "    #         logger.info(f\"Write unlabelled data into rejected file path: [{file_path}]\")\n",
    "    #         unlabelled_dataframe.write.mode(\"append\").parquet(file_path)\n",
    "    #\n",
    "    #         dataframe: DataFrame = dataframe.filter(f\"{self.target_column}!= 'N/A'\")\n",
    "    #\n",
    "    #         logger.info(f\"Remaining data has rows: [{dataframe.count()}] and columns: [{len(dataframe.columns)}]\")\n",
    "    #         return dataframe\n",
    "    #     except Exception as e:\n",
    "    #         raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def initiate_data_validation(self) -> DataValidationArtifact:\n",
    "        try:\n",
    "            logger.info(f\"Initiating data preprocessing.\")\n",
    "            dataframe: DataFrame = self.read_data()\n",
    "            # dataframe = self.drop_row_without_target_label(dataframe=dataframe)\n",
    "\n",
    "            logger.info(f\"Dropping unwanted columns\")\n",
    "            dataframe: DataFrame = self.drop_unwanted_columns(dataframe=dataframe)\n",
    "\n",
    "            # validation to ensure that all require column available\n",
    "            self.is_required_columns_exist(dataframe=dataframe)\n",
    "\n",
    "            logger.info(\"Saving preprocessed data.\")\n",
    "            print(f\"Row: [{dataframe.count()}] Column: [{len(dataframe.columns)}]\")\n",
    "            print(f\"Expected Column: {self.required_columns}\\nPresent Columns: {dataframe.columns}\")\n",
    "\n",
    "            # Create the accepted data directory if it doesn't exist\n",
    "            accepted_dir = Path(self.data_validation_config.accepted_data_dir)\n",
    "            accepted_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Create the accepted file path using Path\n",
    "            accepted_file_path = accepted_dir / self.data_validation_config.file_name\n",
    "\n",
    "            dataframe.write.parquet(accepted_file_path)\n",
    "\n",
    "            artifact = DataValidationArtifact(accepted_file_path=accepted_file_path,\n",
    "                                              rejected_dir=self.data_validation_config.rejected_data_dir\n",
    "                                              )\n",
    "            logger.info(f\"Data validation artifact: [{artifact}]\")\n",
    "            return artifact\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consumerComplaint.components.training.data_ingestion import DataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConsumerComplaintException",
     "evalue": "<module 'sys' (built-in)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConstructorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/utils/main_utils.py\u001b[0m in \u001b[0;36mread_yaml_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/__init__.py\u001b[0m in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSafeLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mget_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_document\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_generators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_object\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag_suffix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_undefined\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;34m\"could not determine a constructor for the tag %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                 node.start_mark)\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConstructorError\u001b[0m: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:collections.OrderedDict'\n  in \"/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/research/consumer_artifact/data_ingestion/meta_info.yaml\", line 1, column 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/entity/metadata_entity.py\u001b[0m in \u001b[0;36mget_metadata_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No metadata file available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_yaml_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mmetadata_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataIngestionMetadataInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/utils/main_utils.py\u001b[0m in \u001b[0;36mread_yaml_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10111/1738090183.py\u001b[0m in \u001b[0;36mget_data_ingestion_config\u001b[0;34m(self, from_date, to_date)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_ingestion_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_metadata_file_present\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mmetadata_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_ingestion_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mfrom_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/entity/metadata_entity.py\u001b[0m in \u001b[0;36mget_metadata_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10111/641652288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinanceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_ingestion_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_ingestion_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_ingestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataIngestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_ingestion_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_ingestion_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_ingestion_artifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_ingestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_data_ingestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10111/1738090183.py\u001b[0m in \u001b[0;36mget_data_ingestion_config\u001b[0;34m(self, from_date, to_date)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>"
     ]
    }
   ],
   "source": [
    "config = FinanceConfig()\n",
    "data_ingestion_config = config.get_data_ingestion_config()\n",
    "data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "data_ingestion_artifact = data_ingestion.initiate_data_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConsumerComplaintException",
     "evalue": "<module 'sys' (built-in)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24045/2652190288.py\u001b[0m in \u001b[0;36mdrop_unwanted_columns\u001b[0;34m(self, dataframe)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrejected_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_validation_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrejected_data_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"missing_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mrejected_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrejected_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_validation_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/research/consumer_artifact/data_validation/20230922_023149/rejected_data/missing_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24045/2652190288.py\u001b[0m in \u001b[0;36minitiate_data_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dropping unwanted columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mdataframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_unwanted_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24045/2652190288.py\u001b[0m in \u001b[0;36mdrop_unwanted_columns\u001b[0;34m(self, dataframe)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24045/2656672851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_validation_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_validation_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataValidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_ingestion_artifact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_ingestion_artifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_validation_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_validation_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_data_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24045/2652190288.py\u001b[0m in \u001b[0;36minitiate_data_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>"
     ]
    }
   ],
   "source": [
    "# config = FinanceConfig()\n",
    "# data_ingestion_artifact = ]()e\n",
    "data_validation_config = config.get_data_validation_config()\n",
    "data_validation = DataValidation(data_ingestion_artifact=data_ingestion_artifact, data_validation_config=data_validation_config, )\n",
    "data_validation.initiate_data_validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark_session.read.parquet(\n",
    "                \"/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/consumer_artifact/data_ingestion/feature_store/consumer_complaint\"\n",
    "            ).limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_10 = dataframe.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+--------------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+--------------+------+--------+\n",
      "|             company|company_public_response|    company_response|complaint_id|complaint_what_happened|consumer_consent_provided|consumer_disputed|       date_received|date_sent_to_company|               issue|             product|state|           sub_issue|         sub_product|submitted_via|          tags|timely|zip_code|\n",
      "+--------------------+-----------------------+--------------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+--------------+------+--------+\n",
      "|TRANSUNION INTERM...|                   null|         In progress|     7529673|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Incorrect informa...|Credit reporting ...|   MI|Old information r...|    Credit reporting|          Web|          null|   Yes|   49022|\n",
      "|TRANSUNION INTERM...|                   null|         In progress|     7553179|                       |                    Other|              N/A|2023-09-15T12:00:...|2023-09-15T12:00:...|Incorrect informa...|Credit reporting ...|   LA|Information belon...|    Credit reporting|          Web|          null|   Yes|   705XX|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7532172|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Incorrect informa...|Credit reporting ...|   MI|Old information r...|    Credit reporting|          Web|          null|   Yes|   49022|\n",
      "|JPMORGAN CHASE & CO.|                   null|Closed with expla...|     7530938|                       |                     null|              N/A|2023-09-12T12:00:...|2023-09-12T12:00:...|Problem with a pu...|         Credit card|   NY|Credit card compa...|General-purpose c...|          Web|          null|   Yes|   11101|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7532205|                       |                    Other|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Improper use of y...|Credit reporting ...|   CO|Credit inquiries ...|    Credit reporting|          Web|          null|   Yes|   805XX|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7531812|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Problem with a co...|Credit reporting ...|   MO|Their investigati...|    Credit reporting|          Web|          null|   Yes|   631XX|\n",
      "|TRANSUNION INTERM...|                   null|         In progress|     7534428|                       |                     null|              N/A|2023-09-12T12:00:...|2023-09-12T12:00:...|Improper use of y...|Credit reporting ...|   SC|Reporting company...|    Credit reporting|          Web|          null|   Yes|   296XX|\n",
      "|BANK OF AMERICA, ...|   Company has respo...|Closed with expla...|     7531882|                       |                      N/A|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...| Managing an account|Checking or savin...|   OR|      Banking errors|    Checking account|        Phone|Older American|   Yes|   97123|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7531558|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Improper use of y...|Credit reporting ...|   RI|Credit inquiries ...|    Credit reporting|          Web|          null|   Yes|   02861|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7531816|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Problem with a co...|Credit reporting ...|   VA|Their investigati...|    Credit reporting|          Web|          null|   Yes|   24551|\n",
      "|TRANSUNION INTERM...|                   null|         In progress|     7530020|                       |                    Other|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Incorrect informa...|Credit reporting ...|   CA|Information belon...|    Credit reporting|          Web|          null|   Yes|   92114|\n",
      "|TRANSUNION INTERM...|                   null|         In progress|     7530023|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Problem with a co...|Credit reporting ...|   MO|Their investigati...|    Credit reporting|          Web|          null|   Yes|   631XX|\n",
      "|WELLS FARGO & COM...|   Company has respo...|Closed with expla...|     7534686|                       |                     null|              N/A|2023-09-12T12:00:...|2023-09-12T12:00:...|Problem caused by...|Checking or savin...|   SC|Overdrafts and ov...|    Checking account|          Web|          null|   Yes|   29640|\n",
      "|BARCLAYS BANK DEL...|   Company has respo...|Closed with expla...|     7540229|                       |     Consent not provided|              N/A|2023-09-12T12:00:...|2023-09-12T12:00:...|Improper use of y...|Credit reporting ...|   MA|Credit inquiries ...|    Credit reporting|          Web|          null|   Yes|   01840|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7562380|                       |                     null|              N/A|2023-09-17T12:00:...|2023-09-17T12:00:...|Improper use of y...|Credit reporting ...|   FL|Credit inquiries ...|    Credit reporting|          Web|          null|   Yes|   34771|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7534199|                       |                     null|              N/A|2023-09-12T12:00:...|2023-09-12T12:00:...|Incorrect informa...|Credit reporting ...|   FL|Account status in...|    Credit reporting|          Web|          null|   Yes|   XXXXX|\n",
      "|TRANSUNION INTERM...|                   null|         In progress|     7527334|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Improper use of y...|Credit reporting ...|   TX|Reporting company...|    Credit reporting|          Web|          null|   Yes|   77477|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7527344|                       |                    Other|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Improper use of y...|Credit reporting ...|   IL|Reporting company...|    Credit reporting|          Web|          null|   Yes|   604XX|\n",
      "|       EQUIFAX, INC.|                   null|         In progress|     7531819|                       |                     null|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Incorrect informa...|Credit reporting ...|   MO|Information belon...|    Credit reporting|          Web|          null|   Yes|   XXXXX|\n",
      "|TRANSUNION INTERM...|                   null|         In progress|     7542281|                       |                     null|              N/A|2023-09-13T12:00:...|2023-09-13T12:00:...|Problem with a co...|         Credit card|   OH|Was not notified ...|General-purpose c...|          Web|          null|   Yes|   45417|\n",
      "+--------------------+-----------------------+--------------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+--------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MissingReport:\n",
    "    total_row : str\n",
    "    missing_row: str\n",
    "    missing_percentage: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIssing REport\n",
    "def get_missing_report(dataframe,):\n",
    "    missing_report = dict()\n",
    "    # logger.info(f\"Preparing missing reports for each column\")\n",
    "    number_of_row = dataframe.count()\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        missing_row = dataframe.filter(f\"{column} is null\").count()\n",
    "        missing_percentage = (missing_row * 100) / number_of_row\n",
    "        missing_report[column] = MissingReport(total_row=number_of_row,\n",
    "                                                missing_row=missing_row,\n",
    "                                                missing_percentage=missing_percentage\n",
    "                                                )\n",
    "    # logger.info(f\"Missing report prepared: {missing_report}\")\n",
    "    return missing_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'company_public_response': MissingReport(total_row=10000, missing_row=9741, missing_percentage=97.41), 'company_response': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'complaint_id': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'complaint_what_happened': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'consumer_consent_provided': MissingReport(total_row=10000, missing_row=8654, missing_percentage=86.54), 'consumer_disputed': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'date_received': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'date_sent_to_company': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'issue': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'product': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'state': MissingReport(total_row=10000, missing_row=8, missing_percentage=0.08), 'sub_issue': MissingReport(total_row=10000, missing_row=68, missing_percentage=0.68), 'sub_product': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'submitted_via': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'tags': MissingReport(total_row=10000, missing_row=9590, missing_percentage=95.9), 'timely': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'zip_code': MissingReport(total_row=10000, missing_row=1, missing_percentage=0.01)}\n"
     ]
    }
   ],
   "source": [
    "print(get_missing_report(dataframe=dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consumerComplaint.config.spark_manager import spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCHEMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from pyspark.sql.types import TimestampType, StringType, StructType, StructField\n",
    "from dataclasses import dataclass\n",
    "from consumerComplaint.exception import ConsumerComplaintException\n",
    "import os, sys\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FeatureProperties:\n",
    "    col_name: str\n",
    "    data_type: str\n",
    "\n",
    "class FinanceDataSchema:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.features = [\n",
    "            FeatureProperties('company_response', 'string'),\n",
    "            FeatureProperties('consumer_consent_provided', 'string'),\n",
    "            FeatureProperties('submitted_via', 'string'),\n",
    "            FeatureProperties('timely', 'string'),\n",
    "            FeatureProperties('date_sent_to_company', 'timestamp'),\n",
    "            FeatureProperties('date_received', 'timestamp'),\n",
    "            FeatureProperties('company', 'string'),\n",
    "            FeatureProperties('issue', 'string'),\n",
    "            FeatureProperties('product', 'string'),\n",
    "            FeatureProperties('state', 'string'),\n",
    "            FeatureProperties('zip_code', 'string'),\n",
    "            FeatureProperties('consumer_disputed', 'string')\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def dataframe_schema(self) -> StructType:\n",
    "        try:\n",
    "            schema = StructType([\n",
    "                StructField(feature.col_name, TimestampType() if feature.data_type == 'timestamp' else StringType())\n",
    "                for feature in self.features\n",
    "            ])\n",
    "            return schema\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys) from e\n",
    "        \n",
    "    @property\n",
    "    def target_column(self) -> str:\n",
    "        return 'consumer_disputed'\n",
    "\n",
    "    @property\n",
    "    def one_hot_encoding_features(self) -> List[str]:\n",
    "        return ['company_response', \n",
    "                'consumer_consent_provided', \n",
    "                'submitted_via']\n",
    "\n",
    "    @property\n",
    "    def im_one_hot_encoding_features(self) -> List[str]:\n",
    "        return [f\"im_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def string_indexer_one_hot_features(self) -> List[str]:\n",
    "        return [f\"si_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def tf_one_hot_encoding_features(self) -> List[str]:\n",
    "        return [f\"tf_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def tfidf_features(self) -> List[str]:\n",
    "        return \"issue\"\n",
    "\n",
    "    @property\n",
    "    def derived_input_features(self) -> List[str]:\n",
    "        features = [\n",
    "            \"date_sent_to_company\",\n",
    "             \"date_received\"\n",
    "        ]\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def derived_output_features(self) -> List[str]:\n",
    "        return [\"diff_in_days\"]\n",
    "\n",
    "    @property\n",
    "    def numerical_columns(self) -> List[str]:\n",
    "        return self.derived_output_features\n",
    "\n",
    "    @property\n",
    "    def im_numerical_columns(self) -> List[str]:\n",
    "        return [f\"im_{col}\" for col in self.numerical_columns]\n",
    "\n",
    "    @property\n",
    "    def tfidf_feature(self) -> List[str]:\n",
    "        return [\"issue\"]\n",
    "\n",
    "    @property\n",
    "    def tf_tfidf_features(self) -> List[str]:\n",
    "        return [f\"tf_{col}\" for col in self.tfidf_feature]\n",
    "\n",
    "    @property\n",
    "    def input_features(self) -> List[str]:\n",
    "        in_features = self.tf_one_hot_encoding_features + self.im_numerical_columns + self.tf_tfidf_features\n",
    "        return in_features\n",
    "\n",
    "    @property\n",
    "    def required_columns(self) -> List[str]:\n",
    "        features = [self.target_column] + self.one_hot_encoding_features + self.tfidf_features + \\\n",
    "                   [\"date_sent_to_company\", \"date_received\"]\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def required_prediction_columns(self) -> List[str]:\n",
    "        features =  self.one_hot_encoding_features + self.tfidf_features + \\\n",
    "                   [\"date_sent_to_company\", \"date_received\"]\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def unwanted_columns(self) -> List[str]:\n",
    "        features = [\"complaint_id\",\n",
    "                    \"sub_product\",  \n",
    "                    \"complaint_what_happened\"]\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def vector_assembler_output(self) -> str:\n",
    "        return \"va_input_features\"\n",
    "\n",
    "    @property\n",
    "    def scaled_vector_input_features(self) -> str:\n",
    "        return \"scaled_input_features\"\n",
    "\n",
    "    @property\n",
    "    def target_indexed_label(self) -> str:\n",
    "        return f\"indexed_{self.target_column}\"\n",
    "\n",
    "    @property\n",
    "    def prediction_column_name(self) -> str:\n",
    "        return \"prediction\"\n",
    "\n",
    "    @property\n",
    "    def prediction_label_column_name(self) -> str:\n",
    "        return f\"{self.prediction_column_name}_{self.target_column}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_schema = FinanceDataSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_schema.dataframe_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altrnative SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql.types import TimestampType, StringType, FloatType, StructType, StructField\n",
    "from  consumerComplaint.exception import ConsumerComplaintException\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class FinanceDataSchema:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.col_company_response: str = 'company_response'\n",
    "        self.col_consumer_consent_provided: str = 'consumer_consent_provided'\n",
    "        self.col_submitted_via = 'submitted_via'\n",
    "        self.col_timely: str = 'timely'\n",
    "        self.col_diff_in_days: str = 'diff_in_days'\n",
    "        self.col_company: str = 'company'\n",
    "        self.col_issue: str = 'issue'\n",
    "        self.col_product: str = 'product'\n",
    "        self.col_state: str = 'state'\n",
    "        self.col_zip_code: str = 'zip_code'\n",
    "        self.col_consumer_disputed: str = 'consumer_disputed'\n",
    "        self.col_date_sent_to_company: str = \"date_sent_to_company\"\n",
    "        self.col_date_received: str = \"date_received\"\n",
    "        self.col_complaint_id: str = \"complaint_id\"\n",
    "        self.col_sub_product: str = \"sub_product\"\n",
    "        self.col_complaint_what_happened: str = \"complaint_what_happened\"\n",
    "        self.col_company_public_response: str = \"company_public_response\"\n",
    "\n",
    "    @property\n",
    "    def dataframe_schema(self) -> StructType:\n",
    "        try:\n",
    "            schema = StructType([\n",
    "                StructField(self.col_company_response, StringType()),\n",
    "                StructField(self.col_consumer_consent_provided, StringType()),\n",
    "                StructField(self.col_submitted_via, StringType()),\n",
    "                StructField(self.col_timely, StringType()),\n",
    "                StructField(self.col_date_sent_to_company, TimestampType()),\n",
    "                StructField(self.col_date_received, TimestampType()),\n",
    "                StructField(self.col_company, StringType()),\n",
    "                StructField(self.col_issue, StringType()),\n",
    "                StructField(self.col_product, StringType()),\n",
    "                StructField(self.col_state, StringType()),\n",
    "                StructField(self.col_zip_code, StringType()),\n",
    "                StructField(self.col_consumer_disputed, StringType()),\n",
    "\n",
    "            ])\n",
    "            return schema\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = FinanceDataSchema()\n",
    "schema = d.dataframe_schema  # Correct way to access the property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "if \"StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))\" == \"StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))\":\n",
    "    print(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
