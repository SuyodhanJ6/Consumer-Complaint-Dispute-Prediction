{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationConfig:\n",
    "    accepted_data_dir: Path\n",
    "    rejected_data_dir: Path\n",
    "    file_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationArtifact:\n",
    "    accepted_file_path: Path\n",
    "    rejected_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consumerComplaint.constants.training_pipeline_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from consumerComplaint.constants.training_pipeline_config.data_validation import *\n",
    "from consumerComplaint.constants.training_pipeline_config import *\n",
    "from consumerComplaint.constants import TIMESTAMP\n",
    "from consumerComplaint.constants import *\n",
    "from consumerComplaint.logger import logger\n",
    "from consumerComplaint.exception import ConsumerComplaintException\n",
    "from consumerComplaint.entity.metadata_entity import DataIngestionMetadata\n",
    "from consumerComplaint.entity.config_entity import DataIngestionConfig, TrainingPipelineConfig, DataValidationConfig\n",
    "\n",
    "\n",
    "\n",
    "class FinanceConfig:\n",
    "    def __init__(self, pipeline_name=PIPELINE_NAME, timestamp=TIMESTAMP):\n",
    "        \"\"\"\n",
    "        Organization: iNeuron Intelligence Private Limited\n",
    "\n",
    "        \"\"\"\n",
    "        self.timestamp = timestamp\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.pipeline_config = self.get_pipeline_config()\n",
    "\n",
    "    def get_pipeline_config(self) -> TrainingPipelineConfig:\n",
    "        \"\"\"\n",
    "        This function will provide pipeline config information\n",
    "\n",
    "\n",
    "        returns > PipelineConfig = namedtuple(\"PipelineConfig\", [\"pipeline_name\", \"artifact_dir\"])\n",
    "        \"\"\"\n",
    "        try:\n",
    "            artifact_dir = PIPELINE_ARTIFACT_DIR\n",
    "            pipeline_config = TrainingPipelineConfig(pipeline_name=self.pipeline_name,\n",
    "                                                     artifact_dir=artifact_dir)\n",
    "\n",
    "            logger.info(f\"Pipeline configuration: {pipeline_config}\")\n",
    "\n",
    "            return pipeline_config\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "\n",
    "    def get_data_ingestion_config(self, \n",
    "                                  from_date=DATA_INGESTION_MIN_START_DATE, \n",
    "                                  to_date=None)-> DataIngestionConfig:\n",
    "        try:\n",
    "            min_start_date = datetime.strptime(DATA_INGESTION_MIN_START_DATE, \"%Y-%m-%d\")\n",
    "            from_date_obj = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "            if from_date_obj < min_start_date:\n",
    "                from_date = DATA_INGESTION_MIN_START_DATE\n",
    "            if to_date is None:\n",
    "                to_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            \"\"\"\n",
    "            master directory for data ingestion\n",
    "            we will store metadata information and ingested file to avoid redundant download\n",
    "            \"\"\"\n",
    "            data_ingestion_master_dir = Path(self.pipeline_config.artifact_dir) / DATA_INGESTION_DIR\n",
    "\n",
    "            # Create a time-based directory for each run\n",
    "            data_ingestion_dir = data_ingestion_master_dir / self.timestamp\n",
    "\n",
    "            metadata_file_path = data_ingestion_master_dir / DATA_INGESTION_METADATA_FILE_NAME\n",
    "\n",
    "            data_ingestion_metadata = DataIngestionMetadata(metadata_file_path=metadata_file_path)\n",
    "\n",
    "            if data_ingestion_metadata.is_metadata_file_present:\n",
    "                metadata_info = data_ingestion_metadata.get_metadata_info()\n",
    "                from_date = metadata_info.to_date\n",
    "\n",
    "            data_ingestion_config = DataIngestionConfig(\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                data_ingestion_dir=data_ingestion_dir,\n",
    "                download_dir=os.path.join(data_ingestion_dir, DATA_INGESTION_DOWNLOADED_DATA_DIR),\n",
    "                file_name=DATA_INGESTION_FILE_NAME,\n",
    "                feature_store_dir=os.path.join(data_ingestion_master_dir, DATA_INGESTION_FEATURE_STORE_DIR),\n",
    "                failed_dir=os.path.join(data_ingestion_dir, DATA_INGESTION_FAILED_DIR),\n",
    "                metadata_file_path=metadata_file_path,\n",
    "                datasource_url=DATA_INGESTION_DATA_SOURCE_URL\n",
    "\n",
    "            )\n",
    "            logger.info(f\"Data ingestion config: {data_ingestion_config}\")\n",
    "            return data_ingestion_config\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "        \n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            data_validation_master_dir = Path(self.pipeline_config.artifact_dir) / DATA_VALIDATION_DIR\n",
    "            data_validation_dir = data_validation_master_dir / self.timestamp\n",
    "\n",
    "            accepted_data_dir = Path(data_validation_dir) / DATA_VALIDATION_ACCEPTED_DATA_DIR\n",
    "            \n",
    "            rejected_data_dir = Path(data_validation_dir) /DATA_VALIDATION_REJECTED_DATA_DIR\n",
    "\n",
    "            data_preprocessing_config = DataValidationConfig(\n",
    "                accepted_data_dir=accepted_data_dir,\n",
    "                rejected_data_dir=rejected_data_dir,\n",
    "                file_name=DATA_VALIDATION_FILE_NAME\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Data preprocessing config: {data_preprocessing_config}\")\n",
    "\n",
    "            return data_preprocessing_config\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FinanceConfig()\n",
    "data_validation_config = config.get_data_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataValidationConfig(accepted_data_dir=PosixPath('/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/research/consumer_artifact/data_validation/20230923_143441/accepted_data'), rejected_data_dir=PosixPath('/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/research/consumer_artifact/data_validation/20230923_143441/rejected_data'), file_name='consumer_complaint')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from typing import List, Dict\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "from consumerComplaint.config.spark_manager import spark_session\n",
    "from consumerComplaint.entity.artifact_entity import DataIngestionArtifact\n",
    "from consumerComplaint.entity.config_entity import DataValidationConfig\n",
    "from consumerComplaint.entity.schema import FinanceDataSchema\n",
    "from consumerComplaint.exception import ConsumerComplaintException\n",
    "from consumerComplaint.logger import logger\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "from consumerComplaint.entity.artifact_entity import DataValidationArtifact\n",
    "\n",
    "COMPLAINT_TABLE = \"complaint\"\n",
    "ERROR_MESSAGE = \"error_msg\"\n",
    "MissingReport = namedtuple(\"MissingReport\", [\"total_row\", \"missing_row\", \"missing_percentage\"])\n",
    "\n",
    "\n",
    "class DataValidation(FinanceDataSchema):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_validation_config: DataValidationConfig,\n",
    "                 data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 table_name: str = COMPLAINT_TABLE,\n",
    "                 schema=FinanceDataSchema()\n",
    "                 ):\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.data_ingestion_artifact: DataIngestionArtifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self.table_name = table_name\n",
    "            self.schema = schema\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys) from e\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        try:\n",
    "            dataframe: DataFrame = spark_session.read.parquet(\n",
    "                self.data_ingestion_artifact.feature_store_file_path\n",
    "            ).limit(10000)\n",
    "            logger.info(f\"Data frame is created using file: {self.data_ingestion_artifact.feature_store_file_path}\")\n",
    "            logger.info(f\"Number of row: {dataframe.count()} and column: {len(dataframe.columns)}\")\n",
    "            #dataframe, _ = dataframe.randomSplit([0.001, 0.999])\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_missing_report(dataframe: DataFrame, ) -> Dict[str, MissingReport]:\n",
    "        try:\n",
    "            missing_report: Dict[str:MissingReport] = dict()\n",
    "            logger.info(f\"Preparing missing reports for each column\")\n",
    "            number_of_row = dataframe.count()\n",
    "\n",
    "            for column in dataframe.columns:\n",
    "                missing_row = dataframe.filter(f\"{column} is null\").count()\n",
    "                missing_percentage = (missing_row * 100) / number_of_row\n",
    "                missing_report[column] = MissingReport(total_row=number_of_row,\n",
    "                                                       missing_row=missing_row,\n",
    "                                                       missing_percentage=missing_percentage\n",
    "                                                       )\n",
    "            logger.info(f\"Missing report prepared: {missing_report}\")\n",
    "            return missing_report\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def get_unwanted_and_high_missing_value_columns(self, dataframe: DataFrame, threshold: float = 0.2) -> List[str]:\n",
    "        try:\n",
    "            missing_report: Dict[str, MissingReport] = self.get_missing_report(dataframe=dataframe)\n",
    "\n",
    "            unwanted_column: List[str] = self.schema.unwanted_columns\n",
    "            for column in missing_report:\n",
    "                if missing_report[column].missing_percentage > (threshold * 100):\n",
    "                    unwanted_column.append(column)\n",
    "                    logger.info(f\"Missing report {column}: [{missing_report[column]}]\")\n",
    "            unwanted_column = list(set(unwanted_column))\n",
    "            return unwanted_column\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def drop_unwanted_columns(self, dataframe: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            unwanted_columns: List = self.get_unwanted_and_high_missing_value_columns(dataframe=dataframe, )\n",
    "            logger.info(f\"Dropping feature: {','.join(unwanted_columns)}\")\n",
    "            unwanted_dataframe: DataFrame = dataframe.select(unwanted_columns)\n",
    "\n",
    "            unwanted_dataframe = unwanted_dataframe.withColumn(ERROR_MESSAGE, lit(\"Contains many missing values\"))\n",
    "\n",
    "            rejected_dir = Path(self.data_validation_config.rejected_data_dir) / \"missing_data\"\n",
    "            rejected_dir.mkdir(exist_ok=True)\n",
    "            file_path =Path(rejected_dir) / self.data_validation_config.file_name\n",
    "\n",
    "            logger.info(f\"Writing dropped column into file: [{file_path}]\")\n",
    "            unwanted_dataframe.write.mode(\"append\").parquet(file_path)\n",
    "            dataframe: DataFrame = dataframe.drop(*unwanted_columns)\n",
    "            logger.info(f\"Remaining number of columns: [{dataframe.columns}]\")\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unique_values_of_each_column(dataframe: DataFrame) -> None:\n",
    "        try:\n",
    "            for column in dataframe.columns:\n",
    "                n_unique: int = dataframe.select(col(column)).distinct().count()\n",
    "                n_missing: int = dataframe.filter(col(column).isNull()).count()\n",
    "                missing_percentage: float = (n_missing * 100) / dataframe.count()\n",
    "                logger.info(f\"Column: {column} contains {n_unique} value and missing perc: {missing_percentage} %.\")\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def is_required_columns_exist(self, dataframe: DataFrame):\n",
    "        try:\n",
    "            columns = list(filter(lambda x: x in self.schema.required_columns,\n",
    "                                  dataframe.columns))\n",
    "\n",
    "            if len(columns) != len(self.schema.required_columns):\n",
    "                raise Exception(f\"Required column missing\\n\\\n",
    "                 Expected columns: {self.schema.required_columns}\\n\\\n",
    "                 Found columns: {columns}\\\n",
    "                 \")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    # def drop_row_without_target_label(self, dataframe: DataFrame) -> DataFrame:\n",
    "    #     try:\n",
    "    #         dropped_rows = \"dropped_row\"\n",
    "    #         total_rows: int = dataframe.count()\n",
    "    #         logger.info(f\"Number of row: {total_rows} \")\n",
    "    #\n",
    "    #         # Drop row if target value is unknown\n",
    "    #         logger.info(f\"Dropping rows without target value.\")\n",
    "    #         unlabelled_dataframe: DataFrame = dataframe.filter(f\"{self.target_column}== 'N/A'\")\n",
    "    #\n",
    "    #         rejected_dir = os.path.join(self.data_validation_config.rejected_data_dir, dropped_rows)\n",
    "    #         os.makedirs(rejected_dir, exist_ok=True)\n",
    "    #         file_path = os.path.join(rejected_dir, self.data_validation_config.file_name)\n",
    "    #\n",
    "    #         unlabelled_dataframe = unlabelled_dataframe.withColumn(ERROR_MESSAGE, lit(\"Dropped row as target label is \"\n",
    "    #                                                                                   \"unknown\"))\n",
    "    #\n",
    "    #         logger.info(f\"Unlabelled data has row: [{unlabelled_dataframe.count()}] and columns:\"\n",
    "    #                     f\" [{len(unlabelled_dataframe.columns)}]\")\n",
    "    #\n",
    "    #         logger.info(f\"Write unlabelled data into rejected file path: [{file_path}]\")\n",
    "    #         unlabelled_dataframe.write.mode(\"append\").parquet(file_path)\n",
    "    #\n",
    "    #         dataframe: DataFrame = dataframe.filter(f\"{self.target_column}!= 'N/A'\")\n",
    "    #\n",
    "    #         logger.info(f\"Remaining data has rows: [{dataframe.count()}] and columns: [{len(dataframe.columns)}]\")\n",
    "    #         return dataframe\n",
    "    #     except Exception as e:\n",
    "    #         raise ConsumerComplaintException(e, sys)\n",
    "\n",
    "    def initiate_data_validation(self) -> DataValidationArtifact:\n",
    "        try:\n",
    "            logger.info(f\"Initiating data preprocessing.\")\n",
    "            dataframe: DataFrame = self.read_data()\n",
    "            # dataframe = self.drop_row_without_target_label(dataframe=dataframe)\n",
    "\n",
    "            logger.info(f\"Dropping unwanted columns\")\n",
    "            dataframe: DataFrame = self.drop_unwanted_columns(dataframe=dataframe)\n",
    "\n",
    "            print(f\"dataframe length: {len(dataframe)}\")\n",
    "\n",
    "            # validation to ensure that all require column available\n",
    "            self.is_required_columns_exist(dataframe=dataframe)\n",
    "\n",
    "            logger.info(\"Saving preprocessed data.\")\n",
    "            print(f\"Row: [{dataframe.count()}] Column: [{len(dataframe.columns)}]\")\n",
    "            print(f\"Expected Column: {self.required_columns}\\nPresent Columns: {dataframe.columns}\")\n",
    "\n",
    "            # Create the accepted data directory if it doesn't exist\n",
    "            accepted_dir = Path(self.data_validation_config.accepted_data_dir)\n",
    "            accepted_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Create the accepted file path using Path\n",
    "            accepted_file_path = accepted_dir / self.data_validation_config.file_name\n",
    "\n",
    "            dataframe.write.parquet(accepted_file_path)\n",
    "\n",
    "            artifact = DataValidationArtifact(accepted_file_path=accepted_file_path,\n",
    "                                              rejected_dir=self.data_validation_config.rejected_data_dir\n",
    "                                              )\n",
    "            logger.info(f\"Data validation artifact: [{artifact}]\")\n",
    "            return artifact\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consumerComplaint.components.training.data_ingestion import DataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConsumerComplaintException",
     "evalue": "<module 'sys' (built-in)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConstructorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/utils/main_utils.py\u001b[0m in \u001b[0;36mread_yaml_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/__init__.py\u001b[0m in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSafeLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mget_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_document\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_generators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_object\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag_suffix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/venv/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_undefined\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;34m\"could not determine a constructor for the tag %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                 node.start_mark)\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConstructorError\u001b[0m: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:collections.OrderedDict'\n  in \"/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/research/consumer_artifact/data_ingestion/meta_info.yaml\", line 1, column 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/entity/metadata_entity.py\u001b[0m in \u001b[0;36mget_metadata_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No metadata file available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_yaml_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"metadata: {metadata}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/utils/main_utils.py\u001b[0m in \u001b[0;36mread_yaml_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39538/1738090183.py\u001b[0m in \u001b[0;36mget_data_ingestion_config\u001b[0;34m(self, from_date, to_date)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_ingestion_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_metadata_file_present\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mmetadata_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_ingestion_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mfrom_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/entity/metadata_entity.py\u001b[0m in \u001b[0;36mget_metadata_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39538/641652288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinanceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_ingestion_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_ingestion_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_ingestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataIngestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_ingestion_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_ingestion_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_ingestion_artifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_ingestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_data_ingestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39538/1738090183.py\u001b[0m in \u001b[0;36mget_data_ingestion_config\u001b[0;34m(self, from_date, to_date)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConsumerComplaintException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConsumerComplaintException\u001b[0m: <module 'sys' (built-in)>"
     ]
    }
   ],
   "source": [
    "config = FinanceConfig()\n",
    "data_ingestion_config = config.get_data_ingestion_config()\n",
    "data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "data_ingestion_artifact = data_ingestion.initiate_data_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_ingestion_artifact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39538/2656672851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# data_ingestion_artifact = ]()e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_validation_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_validation_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataValidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_ingestion_artifact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_ingestion_artifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_validation_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_validation_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_data_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_ingestion_artifact' is not defined"
     ]
    }
   ],
   "source": [
    "# config = FinanceConfig()\n",
    "# data_ingestion_artifact = ]()e\n",
    "data_validation_config = config.get_data_validation_config()\n",
    "data_validation = DataValidation(data_ingestion_artifact=data_ingestion_artifact, data_validation_config=data_validation_config, )\n",
    "data_validation.initiate_data_validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark_session.read.parquet(\n",
    "                \"/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/consumer_artifact/data_ingestion/feature_store/consumer_complaint\"\n",
    "            ).limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_10 = dataframe.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+----------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n",
      "|             company|company_public_response|company_response|complaint_id|complaint_what_happened|consumer_consent_provided|consumer_disputed|       date_received|date_sent_to_company|               issue|             product|state|           sub_issue|         sub_product|submitted_via|tags|timely|zip_code|\n",
      "+--------------------+-----------------------+----------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7550339|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   IL|Information belon...|    Credit reporting|          Web|NULL|   Yes|   60624|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7562824|                       |                     NULL|              N/A|2023-09-17T12:00:...|2023-09-17T12:00:...|Incorrect informa...|Credit reporting ...|   FL|Information belon...|    Credit reporting|          Web|NULL|   Yes|   32168|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7562217|                       |                     NULL|              N/A|2023-09-17T12:00:...|2023-09-17T12:00:...|Problem with a co...|Credit reporting ...|   SC|Their investigati...|    Credit reporting|          Web|NULL|   Yes|   290XX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7562660|                       |                     NULL|              N/A|2023-09-17T12:00:...|2023-09-17T12:00:...|Improper use of y...|Credit reporting ...|   FL|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   33607|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7537189|                       |                     NULL|              N/A|2023-09-13T12:00:...|2023-09-13T12:00:...|Incorrect informa...|Credit reporting ...|   TX|Information belon...|    Credit reporting|          Web|NULL|   Yes|   76522|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551566|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Improper use of y...|Credit reporting ...|   GA|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   XXXXX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551393|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Improper use of y...|Credit reporting ...|   TX|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   75243|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551466|                       |                    Other|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   IL|Information belon...|    Credit reporting|          Web|NULL|   Yes|   60827|\n",
      "|BARCLAYS BANK DEL...|                   NULL|     In progress|     7544334|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Problem with a pu...|         Credit card|   MD|Credit card compa...|   Store credit card|          Web|NULL|   Yes|   209XX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551087|                       |                    Other|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   TX|Information belon...|    Credit reporting|          Web|NULL|   Yes|   75243|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7548924|                       |                    Other|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   GA|Information belon...|    Credit reporting|          Web|NULL|   Yes|   308XX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551910|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   GA|Information belon...|    Credit reporting|          Web|NULL|   Yes|   31705|\n",
      "|Experian Informat...|                   NULL|     In progress|     7556388|                       |                     NULL|              N/A|2023-09-15T12:00:...|2023-09-15T12:00:...|Incorrect informa...|Credit reporting ...|   NC|Information belon...|    Credit reporting|          Web|NULL|   Yes|   28227|\n",
      "| ALLY FINANCIAL INC.|                   NULL|     In progress|     7543335|                       |                     NULL|              N/A|2023-09-13T12:00:...|2023-09-13T12:00:...|Problem with a pu...|         Credit card|   OH|Card was charged ...|General-purpose c...|          Web|NULL|   Yes|   44087|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7529737|                       |                     NULL|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Improper use of y...|Credit reporting ...|   MD|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   20723|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7530225|                       |                    Other|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Incorrect informa...|Credit reporting ...|   TX|Information belon...|    Credit reporting|          Web|NULL|   Yes|   77449|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7527830|                       |                     NULL|              N/A|2023-09-10T12:00:...|2023-09-10T12:00:...|Incorrect informa...|Credit reporting ...|   GA|Account informati...|    Credit reporting|          Web|NULL|   Yes|   30126|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7548576|                       |                     NULL|              N/A|2023-09-15T12:00:...|2023-09-15T12:00:...|Incorrect informa...|Credit reporting ...|   NY|Information belon...|    Credit reporting|          Web|NULL|   Yes|   12582|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7550099|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Problem with a co...|Credit reporting ...|   FL|Their investigati...|    Credit reporting|          Web|NULL|   Yes|   32177|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7549878|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   OH|Information belon...|    Credit reporting|          Web|NULL|   Yes|   44128|\n",
      "+--------------------+-----------------------+----------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MissingReport:\n",
    "    total_row : str\n",
    "    missing_row: str\n",
    "    missing_percentage: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIssing REport\n",
    "def get_missing_report(dataframe,):\n",
    "    missing_report = dict()\n",
    "    # logger.info(f\"Preparing missing reports for each column\")\n",
    "    number_of_row = dataframe.count()\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        missing_row = dataframe.filter(f\"{column} is null\").count()\n",
    "        missing_percentage = (missing_row * 100) / number_of_row\n",
    "        missing_report[column] = MissingReport(total_row=number_of_row,\n",
    "                                                missing_row=missing_row,\n",
    "                                                missing_percentage=missing_percentage\n",
    "                                                )\n",
    "    # logger.info(f\"Missing report prepared: {missing_report}\")\n",
    "    return missing_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'company_public_response': MissingReport(total_row=10000, missing_row=9702, missing_percentage=97.02), 'company_response': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'complaint_id': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'complaint_what_happened': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'consumer_consent_provided': MissingReport(total_row=10000, missing_row=8789, missing_percentage=87.89), 'consumer_disputed': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'date_received': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'date_sent_to_company': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'issue': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'product': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'state': MissingReport(total_row=10000, missing_row=7, missing_percentage=0.07), 'sub_issue': MissingReport(total_row=10000, missing_row=72, missing_percentage=0.72), 'sub_product': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'submitted_via': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'tags': MissingReport(total_row=10000, missing_row=9608, missing_percentage=96.08), 'timely': MissingReport(total_row=10000, missing_row=0, missing_percentage=0.0), 'zip_code': MissingReport(total_row=10000, missing_row=1, missing_percentage=0.01)}\n"
     ]
    }
   ],
   "source": [
    "print(get_missing_report(dataframe=dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consumerComplaint.config.spark_manager import spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCHEMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from pyspark.sql.types import TimestampType, StringType, StructType, StructField\n",
    "from dataclasses import dataclass\n",
    "from consumerComplaint.exception import ConsumerComplaintException\n",
    "import os, sys\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FeatureProperties:\n",
    "    col_name: str\n",
    "    data_type: str\n",
    "\n",
    "class FinanceDataSchema:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.features = [\n",
    "            FeatureProperties('company_response', 'string'),\n",
    "            FeatureProperties('consumer_consent_provided', 'string'),\n",
    "            FeatureProperties('submitted_via', 'string'),\n",
    "            FeatureProperties('timely', 'string'),\n",
    "            FeatureProperties('date_sent_to_company', 'timestamp'),\n",
    "            FeatureProperties('date_received', 'timestamp'),\n",
    "            FeatureProperties('company', 'string'),\n",
    "            FeatureProperties('issue', 'string'),\n",
    "            FeatureProperties('product', 'string'),\n",
    "            FeatureProperties('state', 'string'),\n",
    "            FeatureProperties('zip_code', 'string'),\n",
    "            FeatureProperties('consumer_disputed', 'string')\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def dataframe_schema(self) -> StructType:\n",
    "        try:\n",
    "            schema = StructType([\n",
    "                StructField(feature.col_name, TimestampType() if feature.data_type == 'timestamp' else StringType())\n",
    "                for feature in self.features\n",
    "            ])\n",
    "            return schema\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys) from e\n",
    "        \n",
    "    @property\n",
    "    def target_column(self) -> str:\n",
    "        return 'consumer_disputed'\n",
    "\n",
    "    @property\n",
    "    def one_hot_encoding_features(self) -> List[str]:\n",
    "        return ['company_response', \n",
    "                'consumer_consent_provided', \n",
    "                'submitted_via']\n",
    "\n",
    "    @property\n",
    "    def im_one_hot_encoding_features(self) -> List[str]:\n",
    "        return [f\"im_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def string_indexer_one_hot_features(self) -> List[str]:\n",
    "        return [f\"si_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def tf_one_hot_encoding_features(self) -> List[str]:\n",
    "        return [f\"tf_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def tfidf_features(self) -> List[str]:\n",
    "        return \"issue\"\n",
    "\n",
    "    @property\n",
    "    def derived_input_features(self) -> List[str]:\n",
    "        features = [\n",
    "            \"date_sent_to_company\",\n",
    "             \"date_received\"\n",
    "        ]\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def derived_output_features(self) -> List[str]:\n",
    "        return [\"diff_in_days\"]\n",
    "\n",
    "    @property\n",
    "    def numerical_columns(self) -> List[str]:\n",
    "        return self.derived_output_features\n",
    "\n",
    "    @property\n",
    "    def im_numerical_columns(self) -> List[str]:\n",
    "        return [f\"im_{col}\" for col in self.numerical_columns]\n",
    "\n",
    "    @property\n",
    "    def tfidf_feature(self) -> List[str]:\n",
    "        return [\"issue\"]\n",
    "\n",
    "    @property\n",
    "    def tf_tfidf_features(self) -> List[str]:\n",
    "        return [f\"tf_{col}\" for col in self.tfidf_feature]\n",
    "\n",
    "    @property\n",
    "    def input_features(self) -> List[str]:\n",
    "        in_features = self.tf_one_hot_encoding_features + self.im_numerical_columns + self.tf_tfidf_features\n",
    "        return in_features\n",
    "\n",
    "    @property\n",
    "    def required_columns(self) -> List[str]:\n",
    "        features = [self.target_column] + self.one_hot_encoding_features + self.tfidf_features + \\\n",
    "                   [\"date_sent_to_company\", \"date_received\"]\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def required_prediction_columns(self) -> List[str]:\n",
    "        features =  self.one_hot_encoding_features + self.tfidf_features + \\\n",
    "                   [\"date_sent_to_company\", \"date_received\"]\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def unwanted_columns(self) -> List[str]:\n",
    "        features = [\"complaint_id\",\n",
    "                    \"sub_product\",  \n",
    "                    \"complaint_what_happened\"]\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def vector_assembler_output(self) -> str:\n",
    "        return \"va_input_features\"\n",
    "\n",
    "    @property\n",
    "    def scaled_vector_input_features(self) -> str:\n",
    "        return \"scaled_input_features\"\n",
    "\n",
    "    @property\n",
    "    def target_indexed_label(self) -> str:\n",
    "        return f\"indexed_{self.target_column}\"\n",
    "\n",
    "    @property\n",
    "    def prediction_column_name(self) -> str:\n",
    "        return \"prediction\"\n",
    "\n",
    "    @property\n",
    "    def prediction_label_column_name(self) -> str:\n",
    "        return f\"{self.prediction_column_name}_{self.target_column}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_schema = FinanceDataSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_schema.dataframe_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altrnative SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql.types import TimestampType, StringType, FloatType, StructType, StructField\n",
    "from  consumerComplaint.exception import ConsumerComplaintException\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class FinanceDataSchema:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.col_company_response: str = 'company_response'\n",
    "        self.col_consumer_consent_provided: str = 'consumer_consent_provided'\n",
    "        self.col_submitted_via = 'submitted_via'\n",
    "        self.col_timely: str = 'timely'\n",
    "        self.col_diff_in_days: str = 'diff_in_days'\n",
    "        self.col_company: str = 'company'\n",
    "        self.col_issue: str = 'issue'\n",
    "        self.col_product: str = 'product'\n",
    "        self.col_state: str = 'state'\n",
    "        self.col_zip_code: str = 'zip_code'\n",
    "        self.col_consumer_disputed: str = 'consumer_disputed'\n",
    "        self.col_date_sent_to_company: str = \"date_sent_to_company\"\n",
    "        self.col_date_received: str = \"date_received\"\n",
    "        self.col_complaint_id: str = \"complaint_id\"\n",
    "        self.col_sub_product: str = \"sub_product\"\n",
    "        self.col_complaint_what_happened: str = \"complaint_what_happened\"\n",
    "        self.col_company_public_response: str = \"company_public_response\"\n",
    "\n",
    "    @property\n",
    "    def dataframe_schema(self) -> StructType:\n",
    "        try:\n",
    "            schema = StructType([\n",
    "                StructField(self.col_company_response, StringType()),\n",
    "                StructField(self.col_consumer_consent_provided, StringType()),\n",
    "                StructField(self.col_submitted_via, StringType()),\n",
    "                StructField(self.col_timely, StringType()),\n",
    "                StructField(self.col_date_sent_to_company, TimestampType()),\n",
    "                StructField(self.col_date_received, TimestampType()),\n",
    "                StructField(self.col_company, StringType()),\n",
    "                StructField(self.col_issue, StringType()),\n",
    "                StructField(self.col_product, StringType()),\n",
    "                StructField(self.col_state, StringType()),\n",
    "                StructField(self.col_zip_code, StringType()),\n",
    "                StructField(self.col_consumer_disputed, StringType()),\n",
    "\n",
    "            ])\n",
    "            return schema\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ConsumerComplaintException(e, sys) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = FinanceDataSchema()\n",
    "schema = d.dataframe_schema  # Correct way to access the property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "if \"StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))\" == \"StructType(List(StructField(company_response,StringType,true),StructField(consumer_consent_provided,StringType,true),StructField(submitted_via,StringType,true),StructField(timely,StringType,true),StructField(date_sent_to_company,TimestampType,true),StructField(date_received,TimestampType,true),StructField(company,StringType,true),StructField(issue,StringType,true),StructField(product,StringType,true),StructField(state,StringType,true),StructField(zip_code,StringType,true),StructField(consumer_disputed,StringType,true)))\":\n",
    "    print(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe: DataFrame = spark_session.read.parquet(\n",
    "    \"/home/suyodhan/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/consumer_artifact/data_ingestion/feature_store/consumer_complaint\"\n",
    ").limit(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+----------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n",
      "|             company|company_public_response|company_response|complaint_id|complaint_what_happened|consumer_consent_provided|consumer_disputed|       date_received|date_sent_to_company|               issue|             product|state|           sub_issue|         sub_product|submitted_via|tags|timely|zip_code|\n",
      "+--------------------+-----------------------+----------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7550339|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   IL|Information belon...|    Credit reporting|          Web|NULL|   Yes|   60624|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7562824|                       |                     NULL|              N/A|2023-09-17T12:00:...|2023-09-17T12:00:...|Incorrect informa...|Credit reporting ...|   FL|Information belon...|    Credit reporting|          Web|NULL|   Yes|   32168|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7562217|                       |                     NULL|              N/A|2023-09-17T12:00:...|2023-09-17T12:00:...|Problem with a co...|Credit reporting ...|   SC|Their investigati...|    Credit reporting|          Web|NULL|   Yes|   290XX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7562660|                       |                     NULL|              N/A|2023-09-17T12:00:...|2023-09-17T12:00:...|Improper use of y...|Credit reporting ...|   FL|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   33607|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7537189|                       |                     NULL|              N/A|2023-09-13T12:00:...|2023-09-13T12:00:...|Incorrect informa...|Credit reporting ...|   TX|Information belon...|    Credit reporting|          Web|NULL|   Yes|   76522|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551566|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Improper use of y...|Credit reporting ...|   GA|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   XXXXX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551393|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Improper use of y...|Credit reporting ...|   TX|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   75243|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551466|                       |                    Other|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   IL|Information belon...|    Credit reporting|          Web|NULL|   Yes|   60827|\n",
      "|BARCLAYS BANK DEL...|                   NULL|     In progress|     7544334|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Problem with a pu...|         Credit card|   MD|Credit card compa...|   Store credit card|          Web|NULL|   Yes|   209XX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551087|                       |                    Other|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   TX|Information belon...|    Credit reporting|          Web|NULL|   Yes|   75243|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7548924|                       |                    Other|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   GA|Information belon...|    Credit reporting|          Web|NULL|   Yes|   308XX|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7551910|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   GA|Information belon...|    Credit reporting|          Web|NULL|   Yes|   31705|\n",
      "|Experian Informat...|                   NULL|     In progress|     7556388|                       |                     NULL|              N/A|2023-09-15T12:00:...|2023-09-15T12:00:...|Incorrect informa...|Credit reporting ...|   NC|Information belon...|    Credit reporting|          Web|NULL|   Yes|   28227|\n",
      "| ALLY FINANCIAL INC.|                   NULL|     In progress|     7543335|                       |                     NULL|              N/A|2023-09-13T12:00:...|2023-09-13T12:00:...|Problem with a pu...|         Credit card|   OH|Card was charged ...|General-purpose c...|          Web|NULL|   Yes|   44087|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7529737|                       |                     NULL|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Improper use of y...|Credit reporting ...|   MD|Reporting company...|    Credit reporting|          Web|NULL|   Yes|   20723|\n",
      "|TRANSUNION INTERM...|                   NULL|     In progress|     7530225|                       |                    Other|              N/A|2023-09-11T12:00:...|2023-09-11T12:00:...|Incorrect informa...|Credit reporting ...|   TX|Information belon...|    Credit reporting|          Web|NULL|   Yes|   77449|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7527830|                       |                     NULL|              N/A|2023-09-10T12:00:...|2023-09-10T12:00:...|Incorrect informa...|Credit reporting ...|   GA|Account informati...|    Credit reporting|          Web|NULL|   Yes|   30126|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7548576|                       |                     NULL|              N/A|2023-09-15T12:00:...|2023-09-15T12:00:...|Incorrect informa...|Credit reporting ...|   NY|Information belon...|    Credit reporting|          Web|NULL|   Yes|   12582|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7550099|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Problem with a co...|Credit reporting ...|   FL|Their investigati...|    Credit reporting|          Web|NULL|   Yes|   32177|\n",
      "|       EQUIFAX, INC.|                   NULL|     In progress|     7549878|                       |                     NULL|              N/A|2023-09-14T12:00:...|2023-09-14T12:00:...|Incorrect informa...|Credit reporting ...|   OH|Information belon...|    Credit reporting|          Web|NULL|   Yes|   44128|\n",
      "+--------------------+-----------------------+----------------+------------+-----------------------+-------------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consumerComplaint.entity.schema import FinanceDataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = FinanceDataSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39538/3264114896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Data-Science-Project/Consumer-Complaint-Dispute-Prediction/src/consumerComplaint/entity/schema.py\u001b[0m in \u001b[0;36mrequired_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequired_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_column\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf_features\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                    \u001b[0;34m[\u001b[0m\u001b[0;34m\"date_sent_to_company\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date_received\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "schema.required_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['consumer_disputed'] + ['company_response', \n",
    "                'consumer_consent_provided', \n",
    "                'submitted_via'] + [\"issue\"] + [\"date_sent_to_company\", \"date_received\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consumer_disputed',\n",
       " 'company_response',\n",
       " 'consumer_consent_provided',\n",
       " 'submitted_via',\n",
       " 'issue',\n",
       " 'date_sent_to_company',\n",
       " 'date_received']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
